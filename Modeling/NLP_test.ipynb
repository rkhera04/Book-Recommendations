{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import yake\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rajkhera/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rajkhera/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rajkhera/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/rajkhera/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Bigrams included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_learn_image_detect = \"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\"\n",
    "math_theory_of_comm = \"The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist 1 and Hartley 2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.\"\n",
    "auto_licenseplate_recog_img_process = \"A vehicle license plate recognition system is an important proficiency that could be used for identification of engine vehicle all over the earth. It is valuable in numerous applications such as entrance admission, security, parking control, road traffic control, and speed control. However, the system only manages to identify the license number and needs an operator to control the collected data. Therefore, this paper proposes an automatic license plate recognition system by using the image processing and template matching approach. The current study aims to increase the efficiency of license plate recognition system for Universiti Malaysia Perlis (UniMAP) smart university. This venture comprises of simulation program to recognize license plate characters where a captured image of vehicles will be the input. Then, these images will be processed using several image processing techniques and optical character recognition method in order to recognize the segmented number plate. The image processing techniques consist of colour conversion, image segmentation using Otsu's thresholding, noise removal, image subtraction, image cropping and bounding box feature. The optical character recognition based on template matching approach is used to analyse the printed characters on the segmented license plate image and to produce an output data consisting of characters. Overall, the proposed automatic vehicle license plate recognition system is capable to perform the recognition process by successfully recognizing license plate of 13 cars, from a total of 14 cars.\"\n",
    "stats_rand_signals = \"The autocorrelation and power spectral density functions of a random process are two of the most commonly used concepts in signal processing and in its applications. The relations that define them involve the expected value of a double product of the process or of its Fourier transform. Hence, they are based on second-order statistics. The generalization of this idea leads to the so-called cumulant functions and cumulant spectra, therefore higher-order statistics. Theoretically, the higher-order statistics are null for Gaussian signals. Practically, these quantities are not vanishing. In this paper the third-order statistics for different types of random signals are analyzed.\"\n",
    "matplotlib_2dgraphics = \"Matplotlib is a 2D graphics package used for Python for application development, interactive scripting,and publication-quality image generation across user interfaces and operating systems\"\n",
    "neural_networks_quantization_refine = \"Deploying neural networks (NNs) in low-resource domains is challenging because of their high computing, memory, and power requirements. For this reason, NNs are often quantized before deployment, but such an approach degrades their accuracy. Thus, we propose the counterexample-guided neural network quantization refinement (CEG4N) framework, which combines search-based quantization and equivalence checking. The former minimizes computational requirements, while the latter guarantees that the behavior of an NN does not change after quantization. We evaluate CEG4N on a diverse set of benchmarks, including large and small NNs. Our technique successfully quantizes the networks in the chosen evaluation set, while producing models with up to 163% better accuracy than state-of-the-art techniques.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace hyphens with space\n",
    "    text = text.replace('-', ' ')\n",
    "\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\b\\w*\\d+\\w*\\b', '', text)\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Reassemble text\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 1.0000\n",
      "Original Abstract: Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\n",
      "Preprocessed Abstract: deeper neural network difficult train present residual learning framework ease training network substantially deeper used previously explicitly reformulate layer learning residual function reference layer input instead learning unreferenced function provide comprehensive empirical evidence showing residual network easier optimize gain accuracy considerably increased depth imagenet dataset evaluate residual net depth layer deeper vgg net still lower complexity ensemble residual net achieves error imagenet test set result st place ilsvrc classification task also present analysis cifar layer depth representation central importance many visual recognition task solely due extremely deep representation obtain relative improvement coco object detection dataset deep residual net foundation submission ilsvrc coco competition also st place task imagenet detection imagenet localization coco detection coco segmentation\n",
      "\n",
      "Similarity Score: 0.3941\n",
      "Original Abstract: Deploying neural networks (NNs) in low-resource domains is challenging because of their high computing, memory, and power requirements. For this reason, NNs are often quantized before deployment, but such an approach degrades their accuracy. Thus, we propose the counterexample-guided neural network quantization refinement (CEG4N) framework, which combines search-based quantization and equivalence checking. The former minimizes computational requirements, while the latter guarantees that the behavior of an NN does not change after quantization. We evaluate CEG4N on a diverse set of benchmarks, including large and small NNs. Our technique successfully quantizes the networks in the chosen evaluation set, while producing models with up to 163% better accuracy than state-of-the-art techniques.\n",
      "Preprocessed Abstract: deploying neural network nns low resource domain challenging high computing memory power requirement reason nns often quantized deployment approach degrades accuracy thus propose counterexample guided neural network quantization refinement cegn framework combine search based quantization equivalence checking former minimizes computational requirement latter guarantee behavior nn change quantization evaluate cegn diverse set benchmark including large small nns technique successfully quantizes network chosen evaluation set producing model better accuracy state art technique\n",
      "\n",
      "Similarity Score: 0.1934\n",
      "Original Abstract: A vehicle license plate recognition system is an important proficiency that could be used for identification of engine vehicle all over the earth. It is valuable in numerous applications such as entrance admission, security, parking control, road traffic control, and speed control. However, the system only manages to identify the license number and needs an operator to control the collected data. Therefore, this paper proposes an automatic license plate recognition system by using the image processing and template matching approach. The current study aims to increase the efficiency of license plate recognition system for Universiti Malaysia Perlis (UniMAP) smart university. This venture comprises of simulation program to recognize license plate characters where a captured image of vehicles will be the input. Then, these images will be processed using several image processing techniques and optical character recognition method in order to recognize the segmented number plate. The image processing techniques consist of colour conversion, image segmentation using Otsu's thresholding, noise removal, image subtraction, image cropping and bounding box feature. The optical character recognition based on template matching approach is used to analyse the printed characters on the segmented license plate image and to produce an output data consisting of characters. Overall, the proposed automatic vehicle license plate recognition system is capable to perform the recognition process by successfully recognizing license plate of 13 cars, from a total of 14 cars.\n",
      "Preprocessed Abstract: vehicle license plate recognition system important proficiency could used identification engine vehicle earth valuable numerous application entrance admission security parking control road traffic control speed control however system manages identify license number need operator control collected data therefore paper proposes automatic license plate recognition system using image processing template matching approach current study aim increase efficiency license plate recognition system universiti malaysia perlis unimap smart university venture comprises simulation program recognize license plate character captured image vehicle input image processed using several image processing technique optical character recognition method order recognize segmented number plate image processing technique consist colour conversion image segmentation using otsus thresholding noise removal image subtraction image cropping bounding box feature optical character recognition based template matching approach used analyse printed character segmented license plate image produce output data consisting character overall proposed automatic vehicle license plate recognition system capable perform recognition process successfully recognizing license plate car total car\n",
      "\n",
      "Similarity Score: 0.0741\n",
      "Original Abstract: Matplotlib is a 2D graphics package used for Python for application development, interactive scripting,and publication-quality image generation across user interfaces and operating systems\n",
      "Preprocessed Abstract: matplotlib graphic package used python application development interactive scriptingand publication quality image generation across user interface operating system\n",
      "\n",
      "Similarity Score: 0.0466\n",
      "Original Abstract: The autocorrelation and power spectral density functions of a random process are two of the most commonly used concepts in signal processing and in its applications. The relations that define them involve the expected value of a double product of the process or of its Fourier transform. Hence, they are based on second-order statistics. The generalization of this idea leads to the so-called cumulant functions and cumulant spectra, therefore higher-order statistics. Theoretically, the higher-order statistics are null for Gaussian signals. Practically, these quantities are not vanishing. In this paper the third-order statistics for different types of random signals are analyzed.\n",
      "Preprocessed Abstract: autocorrelation power spectral density function random process two commonly used concept signal processing application relation define involve expected value double product process fourier transform hence based second order statistic generalization idea lead called cumulant function cumulant spectrum therefore higher order statistic theoretically higher order statistic null gaussian signal practically quantity vanishing paper third order statistic different type random signal analyzed\n",
      "\n",
      "Similarity Score: -0.0069\n",
      "Original Abstract: The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist 1 and Hartley 2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.\n",
      "Preprocessed Abstract: recent development various method modulation pcm ppm exchange bandwidth signal noise ratio intensified interest general theory communication basis theory contained important paper nyquist hartley subject present paper extend theory include number new factor particular effect noise channel saving possible due statistical structure original message due nature final destination information\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_embeddings(texts):\n",
    "\n",
    "    preprocessed_texts = [preprocess_text(text) for text in texts]\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(preprocessed_texts)\n",
    "    \n",
    "    # Convert to embeddings using SentenceTransformer\n",
    "    embeddings = model.encode(preprocessed_texts, convert_to_tensor=True)\n",
    "    return embeddings.cpu().numpy()  # Ensure embeddings are on CPU and convert to NumPy array\n",
    "\n",
    "def get_similarities(embedding, embeddings):\n",
    "\n",
    "    return cosine_similarity([embedding], embeddings)[0]\n",
    "\n",
    "def rank_abstracts(query_abstract, abstracts):\n",
    "\n",
    "    # Preprocess texts\n",
    "    preprocessed_abstracts = [preprocess_text(text) for text in abstracts]\n",
    "    preprocessed_query_abstract = preprocess_text(query_abstract)\n",
    "    \n",
    "    # Compute embeddings\n",
    "    all_embeddings = compute_embeddings(abstracts)\n",
    "    query_embedding = compute_embeddings([query_abstract])[0]\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = get_similarities(query_embedding, all_embeddings)\n",
    "\n",
    "    # Get indices of abstracts sorted by similarity\n",
    "    ranked_indices = np.argsort(similarities)[::-1]\n",
    "    \n",
    "    # Return both original and preprocessed abstracts along with similarity scores\n",
    "    return [\n",
    "        (abstracts[i], preprocessed_abstracts[i], similarities[i]) \n",
    "        for i in ranked_indices\n",
    "    ]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # List of research paper abstracts\n",
    "    abstracts = [\n",
    "        deep_learn_image_detect, math_theory_of_comm, auto_licenseplate_recog_img_process, stats_rand_signals, matplotlib_2dgraphics, neural_networks_quantization_refine\n",
    "    ]\n",
    "\n",
    "    # Query abstract\n",
    "    query_abstract = deep_learn_image_detect\n",
    "    \n",
    "    # Get ranked abstracts\n",
    "    ranked_abstracts = rank_abstracts(query_abstract, abstracts)\n",
    "    \n",
    "    # Print results\n",
    "    for original, preprocessed, score in ranked_abstracts:\n",
    "        print(f\"Similarity Score: {score:.4f}\")\n",
    "        print(f\"Original Abstract: {original}\")\n",
    "        print(f\"Preprocessed Abstract: {preprocessed}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
