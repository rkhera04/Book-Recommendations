{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import yake\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rajkhera/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rajkhera/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rajkhera/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/rajkhera/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Bigrams included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = pd.read_csv(\"goodreads_books.csv\", usecols=['Id', 'Name', 'Authors', 'ISBN', 'PublishYear', 'Publisher', 'Language', 'Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39705, 8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Name</th>\n",
       "      <th>Authors</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>PublishYear</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Language</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000</td>\n",
       "      <td>Flight from Eden</td>\n",
       "      <td>Kathryn A. Graham</td>\n",
       "      <td>0595199402</td>\n",
       "      <td>2001</td>\n",
       "      <td>Writer's Showcase Press</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What could a computer expert, a mercenary with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000001</td>\n",
       "      <td>Roommates Again</td>\n",
       "      <td>Kathryn O. Galbraith</td>\n",
       "      <td>0689505973</td>\n",
       "      <td>1994</td>\n",
       "      <td>Margaret K. McElderry Books</td>\n",
       "      <td>NaN</td>\n",
       "      <td>During their stay at Camp Sleep-Away, sisters ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000003</td>\n",
       "      <td>The King At The Door</td>\n",
       "      <td>Brock Cole</td>\n",
       "      <td>0374440417</td>\n",
       "      <td>1992</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A poorly dressed old man appears at an inn and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000004</td>\n",
       "      <td>Giotto: The Scrovegni Chapel, Padua</td>\n",
       "      <td>Bruce Cole</td>\n",
       "      <td>080761310X</td>\n",
       "      <td>1993</td>\n",
       "      <td>George Braziller</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This beautiful series lavishly illustrates the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000005</td>\n",
       "      <td>Larky Mavis</td>\n",
       "      <td>Brock Cole</td>\n",
       "      <td>0374343659</td>\n",
       "      <td>2001</td>\n",
       "      <td>Farrar, Straus and Giroux (BYR)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;b&gt;Another orginal picture-book fairy tale&lt;/b&gt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                 Name               Authors  \\\n",
       "0  1000000                     Flight from Eden     Kathryn A. Graham   \n",
       "1  1000001                      Roommates Again  Kathryn O. Galbraith   \n",
       "2  1000003                 The King At The Door            Brock Cole   \n",
       "3  1000004  Giotto: The Scrovegni Chapel, Padua            Bruce Cole   \n",
       "4  1000005                          Larky Mavis            Brock Cole   \n",
       "\n",
       "         ISBN  PublishYear                        Publisher Language  \\\n",
       "0  0595199402         2001          Writer's Showcase Press      NaN   \n",
       "1  0689505973         1994      Margaret K. McElderry Books      NaN   \n",
       "2  0374440417         1992             Farrar Straus Giroux      NaN   \n",
       "3  080761310X         1993                 George Braziller      NaN   \n",
       "4  0374343659         2001  Farrar, Straus and Giroux (BYR)      NaN   \n",
       "\n",
       "                                         Description  \n",
       "0  What could a computer expert, a mercenary with...  \n",
       "1  During their stay at Camp Sleep-Away, sisters ...  \n",
       "2  A poorly dressed old man appears at an inn and...  \n",
       "3  This beautiful series lavishly illustrates the...  \n",
       "4  <b>Another orginal picture-book fairy tale</b>...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows where description is NA\n",
    "\n",
    "books_cleaned_df = books_df.dropna(subset=['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34559, 8)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(books_cleaned_df['Description'].isna())\n",
    "books_cleaned_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"\"\"\n",
    "        This study presents a comparison of different deep learning methods used for sentiment analysis \n",
    "        in Twitter data. In this domain, deep learning (DL) techniques, which contribute at the same time \n",
    "        to the solution of a wide range of problems, gained popularity among researchers. Particularly, two \n",
    "        categories of neural networks are utilized, convolutional neural networks(CNN), which are especially \n",
    "        performant in the area of image processing and recurrent neural networks (RNN) which are applied with \n",
    "        success in natural language processing (NLP) tasks. In this work we evaluate and compare ensembles \n",
    "        and combinations of CNN and a category of RNN the long short-term memory (LSTM) networks. Additionally,\n",
    "        we compare different word embedding systems such as the Word2Vec and the global vectors for word \n",
    "        representation (GloVe) models. For the evaluation of those methods we used data provided by the international \n",
    "        workshop on semantic evaluation (SemEval), which is one of the most popular international workshops \n",
    "        on the area. Various tests and combinations are applied and best scoring values for each model are \n",
    "        compared in terms of their performance. This study contributes to the field of sentiment analysis by \n",
    "        analyzing the performances, advantages and limitations of the above methods with an evaluation procedure \n",
    "        under a single testing framework with the same dataset and computing environment.\n",
    "      \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = \"\"\"\n",
    "        The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly \n",
    "        and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for \n",
    "        statistical model identification. The classical maximum likelihood estimation procedure is reviewed and a \n",
    "        new estimate minimum information theoretical criterion (AIC) estimate (MAICE) which is designed for the purpose \n",
    "        of statistical identification is introduced. When there are several competing models the MAICE is defined \n",
    "        by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined \n",
    "        by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE \n",
    "        provides a versatile procedure for statistical model identification which is free from the ambiguities inherent \n",
    "        in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time \n",
    "        series analysis is demonstrated with some numerical examples.\n",
    "       \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words_from_keywords(keywords, stop_words):\n",
    "    filtered_keywords = []\n",
    "    for keyword in keywords:\n",
    "        # Check if any word in the keyword phrase is a stop word\n",
    "        if not any(word.lower() in stop_words for word in keyword[0].split()):\n",
    "            filtered_keywords.append(keyword)\n",
    "    return filtered_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"en\"\n",
    "max_ngram_size = 3\n",
    "deduplication_threshold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 1\n",
    "numOfKeywords = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('deep learning', 0.022316030222952542)\n",
      "('deep learning methods', 0.023033274943689574)\n",
      "('Twitter data', 0.027097682190152975)\n",
      "('neural networks', 0.036406670620403876)\n",
      "('Twitter', 0.06453868188482255)\n",
      "('convolutional neural networks', 0.06465373368643157)\n",
      "('recurrent neural networks', 0.07055699871496693)\n",
      "('study presents', 0.07282416883388629)\n"
     ]
    }
   ],
   "source": [
    "keywords = custom_kw_extractor.extract_keywords(doc1)\n",
    "filtered_keywords = remove_stop_words_from_keywords(keywords, stop_words)\n",
    "for kw in filtered_keywords:\n",
    "\tprint(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('statistical model identification', 0.010789442479580198)\n",
      "('hypothesis testing procedure', 0.012530368145606251)\n",
      "('statistical hypothesis testing', 0.02540919367201189)\n",
      "('time series analysis', 0.032893935889892506)\n",
      "('hypothesis testing', 0.03340942932098045)\n",
      "('testing procedure', 0.034824999228042636)\n",
      "('maximum likelihood', 0.03631424036749724)\n",
      "('statistical model', 0.03662978375322814)\n",
      "('likelihood estimation procedure', 0.04035102042239439)\n",
      "('model identification', 0.042820350337371825)\n"
     ]
    }
   ],
   "source": [
    "keywords = custom_kw_extractor.extract_keywords(doc2)\n",
    "filtered_keywords = remove_stop_words_from_keywords(keywords, stop_words)\n",
    "for kw in filtered_keywords:\n",
    "\tprint(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_learn_image_detect = \"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\"\n",
    "math_theory_of_comm = \"The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist 1 and Hartley 2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.\"\n",
    "auto_licenseplate_recog_img_process = \"A vehicle license plate recognition system is an important proficiency that could be used for identification of engine vehicle all over the earth. It is valuable in numerous applications such as entrance admission, security, parking control, road traffic control, and speed control. However, the system only manages to identify the license number and needs an operator to control the collected data. Therefore, this paper proposes an automatic license plate recognition system by using the image processing and template matching approach. The current study aims to increase the efficiency of license plate recognition system for Universiti Malaysia Perlis (UniMAP) smart university. This venture comprises of simulation program to recognize license plate characters where a captured image of vehicles will be the input. Then, these images will be processed using several image processing techniques and optical character recognition method in order to recognize the segmented number plate. The image processing techniques consist of colour conversion, image segmentation using Otsu's thresholding, noise removal, image subtraction, image cropping and bounding box feature. The optical character recognition based on template matching approach is used to analyse the printed characters on the segmented license plate image and to produce an output data consisting of characters. Overall, the proposed automatic vehicle license plate recognition system is capable to perform the recognition process by successfully recognizing license plate of 13 cars, from a total of 14 cars.\"\n",
    "stats_rand_signals = \"The autocorrelation and power spectral density functions of a random process are two of the most commonly used concepts in signal processing and in its applications. The relations that define them involve the expected value of a double product of the process or of its Fourier transform. Hence, they are based on second-order statistics. The generalization of this idea leads to the so-called cumulant functions and cumulant spectra, therefore higher-order statistics. Theoretically, the higher-order statistics are null for Gaussian signals. Practically, these quantities are not vanishing. In this paper the third-order statistics for different types of random signals are analyzed.\"\n",
    "matplotlib_2dgraphics = \"Matplotlib is a 2D graphics package used for Python for application development, interactive scripting,and publication-quality image generation across user interfaces and operating systems\"\n",
    "neural_networks_quantization_refine = \"Deploying neural networks (NNs) in low-resource domains is challenging because of their high computing, memory, and power requirements. For this reason, NNs are often quantized before deployment, but such an approach degrades their accuracy. Thus, we propose the counterexample-guided neural network quantization refinement (CEG4N) framework, which combines search-based quantization and equivalence checking. The former minimizes computational requirements, while the latter guarantees that the behavior of an NN does not change after quantization. We evaluate CEG4N on a diverse set of benchmarks, including large and small NNs. Our technique successfully quantizes the networks in the chosen evaluation set, while producing models with up to 163% better accuracy than state-of-the-art techniques.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace hyphens with space\n",
    "    text = text.replace('-', ' ')\n",
    "\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\b\\w*\\d+\\w*\\b', '', text)\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Reassemble text\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_scores(scores):\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    return scaler.fit_transform(scores.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 1.0000\n",
      "Original Abstract: Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\n",
      "Preprocessed Abstract: deeper neural network difficult train present residual learning framework ease training network substantially deeper used previously explicitly reformulate layer learning residual function reference layer input instead learning unreferenced function provide comprehensive empirical evidence showing residual network easier optimize gain accuracy considerably increased depth imagenet dataset evaluate residual net depth layer deeper vgg net still lower complexity ensemble residual net achieves error imagenet test set result st place ilsvrc classification task also present analysis cifar layer depth representation central importance many visual recognition task solely due extremely deep representation obtain relative improvement coco object detection dataset deep residual net foundation submission ilsvrc coco competition also st place task imagenet detection imagenet localization coco detection coco segmentation\n",
      "\n",
      "Similarity Score: 0.3941\n",
      "Original Abstract: Deploying neural networks (NNs) in low-resource domains is challenging because of their high computing, memory, and power requirements. For this reason, NNs are often quantized before deployment, but such an approach degrades their accuracy. Thus, we propose the counterexample-guided neural network quantization refinement (CEG4N) framework, which combines search-based quantization and equivalence checking. The former minimizes computational requirements, while the latter guarantees that the behavior of an NN does not change after quantization. We evaluate CEG4N on a diverse set of benchmarks, including large and small NNs. Our technique successfully quantizes the networks in the chosen evaluation set, while producing models with up to 163% better accuracy than state-of-the-art techniques.\n",
      "Preprocessed Abstract: deploying neural network nns low resource domain challenging high computing memory power requirement reason nns often quantized deployment approach degrades accuracy thus propose counterexample guided neural network quantization refinement cegn framework combine search based quantization equivalence checking former minimizes computational requirement latter guarantee behavior nn change quantization evaluate cegn diverse set benchmark including large small nns technique successfully quantizes network chosen evaluation set producing model better accuracy state art technique\n",
      "\n",
      "Similarity Score: 0.1934\n",
      "Original Abstract: A vehicle license plate recognition system is an important proficiency that could be used for identification of engine vehicle all over the earth. It is valuable in numerous applications such as entrance admission, security, parking control, road traffic control, and speed control. However, the system only manages to identify the license number and needs an operator to control the collected data. Therefore, this paper proposes an automatic license plate recognition system by using the image processing and template matching approach. The current study aims to increase the efficiency of license plate recognition system for Universiti Malaysia Perlis (UniMAP) smart university. This venture comprises of simulation program to recognize license plate characters where a captured image of vehicles will be the input. Then, these images will be processed using several image processing techniques and optical character recognition method in order to recognize the segmented number plate. The image processing techniques consist of colour conversion, image segmentation using Otsu's thresholding, noise removal, image subtraction, image cropping and bounding box feature. The optical character recognition based on template matching approach is used to analyse the printed characters on the segmented license plate image and to produce an output data consisting of characters. Overall, the proposed automatic vehicle license plate recognition system is capable to perform the recognition process by successfully recognizing license plate of 13 cars, from a total of 14 cars.\n",
      "Preprocessed Abstract: vehicle license plate recognition system important proficiency could used identification engine vehicle earth valuable numerous application entrance admission security parking control road traffic control speed control however system manages identify license number need operator control collected data therefore paper proposes automatic license plate recognition system using image processing template matching approach current study aim increase efficiency license plate recognition system universiti malaysia perlis unimap smart university venture comprises simulation program recognize license plate character captured image vehicle input image processed using several image processing technique optical character recognition method order recognize segmented number plate image processing technique consist colour conversion image segmentation using otsus thresholding noise removal image subtraction image cropping bounding box feature optical character recognition based template matching approach used analyse printed character segmented license plate image produce output data consisting character overall proposed automatic vehicle license plate recognition system capable perform recognition process successfully recognizing license plate car total car\n",
      "\n",
      "Similarity Score: 0.0522\n",
      "Original Abstract: Matplotlib is a 2D graphics package used for Python for application development, interactive scripting, and publication-quality image generation across user interfaces and operating systems\n",
      "Preprocessed Abstract: matplotlib graphic package used python application development interactive scripting publication quality image generation across user interface operating system\n",
      "\n",
      "Similarity Score: 0.0466\n",
      "Original Abstract: The autocorrelation and power spectral density functions of a random process are two of the most commonly used concepts in signal processing and in its applications. The relations that define them involve the expected value of a double product of the process or of its Fourier transform. Hence, they are based on second-order statistics. The generalization of this idea leads to the so-called cumulant functions and cumulant spectra, therefore higher-order statistics. Theoretically, the higher-order statistics are null for Gaussian signals. Practically, these quantities are not vanishing. In this paper the third-order statistics for different types of random signals are analyzed.\n",
      "Preprocessed Abstract: autocorrelation power spectral density function random process two commonly used concept signal processing application relation define involve expected value double product process fourier transform hence based second order statistic generalization idea lead called cumulant function cumulant spectrum therefore higher order statistic theoretically higher order statistic null gaussian signal practically quantity vanishing paper third order statistic different type random signal analyzed\n",
      "\n",
      "Similarity Score: -0.0069\n",
      "Original Abstract: The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist 1 and Hartley 2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.\n",
      "Preprocessed Abstract: recent development various method modulation pcm ppm exchange bandwidth signal noise ratio intensified interest general theory communication basis theory contained important paper nyquist hartley subject present paper extend theory include number new factor particular effect noise channel saving possible due statistical structure original message due nature final destination information\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_embeddings(texts):\n",
    "\n",
    "    preprocessed_texts = [preprocess_text(text) for text in texts]\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(preprocessed_texts)\n",
    "    \n",
    "    # Convert to embeddings using SentenceTransformer\n",
    "    embeddings = model.encode(preprocessed_texts, convert_to_tensor=True)\n",
    "    return embeddings.cpu().numpy()  # Ensure embeddings are on CPU and convert to NumPy array\n",
    "\n",
    "def get_similarities(embedding, embeddings):\n",
    "\n",
    "    return cosine_similarity([embedding], embeddings)[0]\n",
    "\n",
    "def rank_abstracts(query_abstract, abstracts):\n",
    "\n",
    "    # Preprocess texts\n",
    "    preprocessed_abstracts = [preprocess_text(text) for text in abstracts]\n",
    "    preprocessed_query_abstract = preprocess_text(query_abstract)\n",
    "    \n",
    "    # Compute embeddings\n",
    "    all_embeddings = compute_embeddings(abstracts)\n",
    "    query_embedding = compute_embeddings([query_abstract])[0]\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = get_similarities(query_embedding, all_embeddings)\n",
    "    \n",
    "    # Get indices of abstracts sorted by similarity\n",
    "    ranked_indices = np.argsort(similarities)[::-1]\n",
    "    \n",
    "    # Return both original and preprocessed abstracts along with similarity scores\n",
    "    return [\n",
    "        (abstracts[i], preprocessed_abstracts[i], similarities[i]) \n",
    "        for i in ranked_indices\n",
    "    ]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # List of research paper abstracts\n",
    "    abstracts = [\n",
    "        deep_learn_image_detect, math_theory_of_comm, auto_licenseplate_recog_img_process, stats_rand_signals, matplotlib_2dgraphics, neural_networks_quantization_refine\n",
    "    ]\n",
    "\n",
    "    # Query abstract\n",
    "    query_abstract = deep_learn_image_detect\n",
    "    \n",
    "    # Get ranked abstracts\n",
    "    ranked_abstracts = rank_abstracts(query_abstract, abstracts)\n",
    "    \n",
    "    # Print results\n",
    "    for original, preprocessed, score in ranked_abstracts:\n",
    "        print(f\"Similarity Score: {score:.4f}\")\n",
    "        print(f\"Original Abstract: {original}\")\n",
    "        print(f\"Preprocessed Abstract: {preprocessed}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
