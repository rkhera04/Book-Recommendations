{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rajkhera/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/rajkhera/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = pd.read_csv(\"goodreads_books.csv\", usecols=['Id', 'Name', 'Authors', 'ISBN', 'PublishYear', 'Publisher', 'Language', 'Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39705, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Name</th>\n",
       "      <th>Authors</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>PublishYear</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Language</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000</td>\n",
       "      <td>Flight from Eden</td>\n",
       "      <td>Kathryn A. Graham</td>\n",
       "      <td>0595199402</td>\n",
       "      <td>2001</td>\n",
       "      <td>Writer's Showcase Press</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What could a computer expert, a mercenary with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000001</td>\n",
       "      <td>Roommates Again</td>\n",
       "      <td>Kathryn O. Galbraith</td>\n",
       "      <td>0689505973</td>\n",
       "      <td>1994</td>\n",
       "      <td>Margaret K. McElderry Books</td>\n",
       "      <td>NaN</td>\n",
       "      <td>During their stay at Camp Sleep-Away, sisters ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000003</td>\n",
       "      <td>The King At The Door</td>\n",
       "      <td>Brock Cole</td>\n",
       "      <td>0374440417</td>\n",
       "      <td>1992</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A poorly dressed old man appears at an inn and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000004</td>\n",
       "      <td>Giotto: The Scrovegni Chapel, Padua</td>\n",
       "      <td>Bruce Cole</td>\n",
       "      <td>080761310X</td>\n",
       "      <td>1993</td>\n",
       "      <td>George Braziller</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This beautiful series lavishly illustrates the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000005</td>\n",
       "      <td>Larky Mavis</td>\n",
       "      <td>Brock Cole</td>\n",
       "      <td>0374343659</td>\n",
       "      <td>2001</td>\n",
       "      <td>Farrar, Straus and Giroux (BYR)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;b&gt;Another orginal picture-book fairy tale&lt;/b&gt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                 Name               Authors  \\\n",
       "0  1000000                     Flight from Eden     Kathryn A. Graham   \n",
       "1  1000001                      Roommates Again  Kathryn O. Galbraith   \n",
       "2  1000003                 The King At The Door            Brock Cole   \n",
       "3  1000004  Giotto: The Scrovegni Chapel, Padua            Bruce Cole   \n",
       "4  1000005                          Larky Mavis            Brock Cole   \n",
       "\n",
       "         ISBN  PublishYear                        Publisher Language  \\\n",
       "0  0595199402         2001          Writer's Showcase Press      NaN   \n",
       "1  0689505973         1994      Margaret K. McElderry Books      NaN   \n",
       "2  0374440417         1992             Farrar Straus Giroux      NaN   \n",
       "3  080761310X         1993                 George Braziller      NaN   \n",
       "4  0374343659         2001  Farrar, Straus and Giroux (BYR)      NaN   \n",
       "\n",
       "                                         Description  \n",
       "0  What could a computer expert, a mercenary with...  \n",
       "1  During their stay at Camp Sleep-Away, sisters ...  \n",
       "2  A poorly dressed old man appears at an inn and...  \n",
       "3  This beautiful series lavishly illustrates the...  \n",
       "4  <b>Another orginal picture-book fairy tale</b>...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows where description is NA\n",
    "\n",
    "books_cleaned_df = books_df.dropna(subset=['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34559, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(books_cleaned_df['Description'].isna())\n",
    "books_cleaned_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"\"\"\n",
    "        This study presents a comparison of different deep learning methods used for sentiment analysis \n",
    "        in Twitter data. In this domain, deep learning (DL) techniques, which contribute at the same time \n",
    "        to the solution of a wide range of problems, gained popularity among researchers. Particularly, two \n",
    "        categories of neural networks are utilized, convolutional neural networks(CNN), which are especially \n",
    "        performant in the area of image processing and recurrent neural networks (RNN) which are applied with \n",
    "        success in natural language processing (NLP) tasks. In this work we evaluate and compare ensembles \n",
    "        and combinations of CNN and a category of RNN the long short-term memory (LSTM) networks. Additionally,\n",
    "        we compare different word embedding systems such as the Word2Vec and the global vectors for word \n",
    "        representation (GloVe) models. For the evaluation of those methods we used data provided by the international \n",
    "        workshop on semantic evaluation (SemEval), which is one of the most popular international workshops \n",
    "        on the area. Various tests and combinations are applied and best scoring values for each model are \n",
    "        compared in terms of their performance. This study contributes to the field of sentiment analysis by \n",
    "        analyzing the performances, advantages and limitations of the above methods with an evaluation procedure \n",
    "        under a single testing framework with the same dataset and computing environment.\n",
    "      \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = \"\"\"\n",
    "        The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly \n",
    "        and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for \n",
    "        statistical model identification. The classical maximum likelihood estimation procedure is reviewed and a \n",
    "        new estimate minimum information theoretical criterion (AIC) estimate (MAICE) which is designed for the purpose \n",
    "        of statistical identification is introduced. When there are several competing models the MAICE is defined \n",
    "        by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined \n",
    "        by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE \n",
    "        provides a versatile procedure for statistical model identification which is free from the ambiguities inherent \n",
    "        in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time \n",
    "        series analysis is demonstrated with some numerical examples.\n",
    "       \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words_from_keywords(keywords, stop_words):\n",
    "    filtered_keywords = []\n",
    "    for keyword in keywords:\n",
    "        # Check if any word in the keyword phrase is a stop word\n",
    "        if not any(word.lower() in stop_words for word in keyword[0].split()):\n",
    "            filtered_keywords.append(keyword)\n",
    "    return filtered_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"en\"\n",
    "max_ngram_size = 3\n",
    "deduplication_threshold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 1\n",
    "numOfKeywords = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('deep learning', 0.022316030222952542)\n",
      "('deep learning methods', 0.023033274943689574)\n",
      "('Twitter data', 0.027097682190152975)\n",
      "('neural networks', 0.036406670620403876)\n",
      "('Twitter', 0.06453868188482255)\n",
      "('convolutional neural networks', 0.06465373368643157)\n",
      "('recurrent neural networks', 0.07055699871496693)\n",
      "('study presents', 0.07282416883388629)\n"
     ]
    }
   ],
   "source": [
    "keywords = custom_kw_extractor.extract_keywords(doc1)\n",
    "filtered_keywords = remove_stop_words_from_keywords(keywords, stop_words)\n",
    "for kw in filtered_keywords:\n",
    "\tprint(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('statistical model identification', 0.010789442479580198)\n",
      "('hypothesis testing procedure', 0.012530368145606251)\n",
      "('statistical hypothesis testing', 0.02540919367201189)\n",
      "('time series analysis', 0.032893935889892506)\n",
      "('hypothesis testing', 0.03340942932098045)\n",
      "('testing procedure', 0.034824999228042636)\n",
      "('maximum likelihood', 0.03631424036749724)\n",
      "('statistical model', 0.03662978375322814)\n",
      "('likelihood estimation procedure', 0.04035102042239439)\n",
      "('model identification', 0.042820350337371825)\n"
     ]
    }
   ],
   "source": [
    "keywords = custom_kw_extractor.extract_keywords(doc2)\n",
    "filtered_keywords = remove_stop_words_from_keywords(keywords, stop_words)\n",
    "for kw in filtered_keywords:\n",
    "\tprint(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajkhera/Book-Recommendations/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/rajkhera/Book-Recommendations/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Example usage\n",
    "embeddings = [get_embeddings(abstract) for abstract in doc1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_learn_image_detect = \"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\"\n",
    "math_theory_of_comm = \"The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist 1 and Hartley 2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.\"\n",
    "auto_licenseplate_recog_img_process = \"A vehicle license plate recognition system is an important proficiency that could be used for identification of engine vehicle all over the earth. It is valuable in numerous applications such as entrance admission, security, parking control, road traffic control, and speed control. However, the system only manages to identify the license number and needs an operator to control the collected data. Therefore, this paper proposes an automatic license plate recognition system by using the image processing and template matching approach. The current study aims to increase the efficiency of license plate recognition system for Universiti Malaysia Perlis (UniMAP) smart university. This venture comprises of simulation program to recognize license plate characters where a captured image of vehicles will be the input. Then, these images will be processed using several image processing techniques and optical character recognition method in order to recognize the segmented number plate. The image processing techniques consist of colour conversion, image segmentation using Otsu's thresholding, noise removal, image subtraction, image cropping and bounding box feature. The optical character recognition based on template matching approach is used to analyse the printed characters on the segmented license plate image and to produce an output data consisting of characters. Overall, the proposed automatic vehicle license plate recognition system is capable to perform the recognition process by successfully recognizing license plate of 13 cars, from a total of 14 cars.\"\n",
    "stats_rand_signals = \"The autocorrelation and power spectral density functions of a random process are two of the most commonly used concepts in signal processing and in its applications. The relations that define them involve the expected value of a double product of the process or of its Fourier transform. Hence, they are based on second-order statistics. The generalization of this idea leads to the so-called cumulant functions and cumulant spectra, therefore higher-order statistics. Theoretically, the higher-order statistics are null for Gaussian signals. Practically, these quantities are not vanishing. In this paper the third-order statistics for different types of random signals are analyzed.\"\n",
    "matplotlib_2dgraphics = \"Matplotlib is a 2D graphics package used for Python for application development, interactive scripting,and publication-quality image generation across user interfaces and operating systems\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 1.0000\n",
      "Abstract: Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\n",
      "\n",
      "Similarity Score: 0.0587\n",
      "Abstract: A vehicle license plate recognition system is an important proficiency that could be used for identification of engine vehicle all over the earth. It is valuable in numerous applications such as entrance admission, security, parking control, road traffic control, and speed control. However, the system only manages to identify the license number and needs an operator to control the collected data. Therefore, this paper proposes an automatic license plate recognition system by using the image processing and template matching approach. The current study aims to increase the efficiency of license plate recognition system for Universiti Malaysia Perlis (UniMAP) smart university. This venture comprises of simulation program to recognize license plate characters where a captured image of vehicles will be the input. Then, these images will be processed using several image processing techniques and optical character recognition method in order to recognize the segmented number plate. The image processing techniques consist of colour conversion, image segmentation using Otsu's thresholding, noise removal, image subtraction, image cropping and bounding box feature. The optical character recognition based on template matching approach is used to analyse the printed characters on the segmented license plate image and to produce an output data consisting of characters. Overall, the proposed automatic vehicle license plate recognition system is capable to perform the recognition process by successfully recognizing license plate of 13 cars, from a total of 14 cars.\n",
      "\n",
      "Similarity Score: -0.0112\n",
      "Abstract: Matplotlib is a 2D graphics package used for Python for application development, interactive scripting,and publication-quality image generation across user interfaces and operating systems\n",
      "\n",
      "Similarity Score: -0.0170\n",
      "Abstract: The autocorrelation and power spectral density functions of a random process are two of the most commonly used concepts in signal processing and in its applications. The relations that define them involve the expected value of a double product of the process or of its Fourier transform. Hence, they are based on second-order statistics. The generalization of this idea leads to the so-called cumulant functions and cumulant spectra, therefore higher-order statistics. Theoretically, the higher-order statistics are null for Gaussian signals. Practically, these quantities are not vanishing. In this paper the third-order statistics for different types of random signals are analyzed.\n",
      "\n",
      "Similarity Score: -0.0923\n",
      "Abstract: The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist 1 and Hartley 2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def compute_embeddings(texts):\n",
    "    \"\"\"Compute embeddings for a list of texts.\"\"\"\n",
    "    embeddings = model.encode(texts, convert_to_tensor=True)\n",
    "    return embeddings.cpu().numpy()  # Ensure embeddings are on CPU and convert to NumPy array\n",
    "\n",
    "def get_similarities(embedding, embeddings):\n",
    "    \"\"\"Compute cosine similarities between one embedding and a list of embeddings.\"\"\"\n",
    "    return cosine_similarity([embedding], embeddings)[0]\n",
    "\n",
    "def rank_abstracts(query_abstract, abstracts):\n",
    "    \"\"\"Rank abstracts by similarity to the query abstract.\"\"\"\n",
    "    # Compute embeddings\n",
    "    all_embeddings = compute_embeddings(abstracts)\n",
    "    query_embedding = compute_embeddings([query_abstract])[0]\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = get_similarities(query_embedding, all_embeddings)\n",
    "    \n",
    "    # Get indices of abstracts sorted by similarity\n",
    "    ranked_indices = np.argsort(similarities)[::-1]\n",
    "    \n",
    "    return [(abstracts[i], similarities[i]) for i in ranked_indices]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # List of research paper abstracts\n",
    "    abstracts = [\n",
    "        deep_learn_image_detect, math_theory_of_comm, auto_licenseplate_recog_img_process, stats_rand_signals, matplotlib_2dgraphics\n",
    "    ]\n",
    "    \n",
    "    # Query abstract\n",
    "    query_abstract = deep_learn_image_detect\n",
    "    \n",
    "    # Get ranked abstracts\n",
    "    ranked_abstracts = rank_abstracts(query_abstract, abstracts)\n",
    "    \n",
    "    # Print results\n",
    "    for abstract, score in ranked_abstracts:\n",
    "        print(f\"Similarity Score: {score:.4f}\\nAbstract: {abstract}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- remove stop words\n",
    "- stemming \n",
    "- tokenization \n",
    "- \n",
    "- try different vectorization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
